{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 CIFAR 10  Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn import metrics\n",
    "import os \n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def compute_confusion_matrix(target, pred, normalize=None):\n",
    "    return metrics.confusion_matrix(\n",
    "        target.detach().cpu().numpy(), \n",
    "        pred.detach().cpu().numpy(),\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "def show_image(img):\n",
    "    img = img.detach().cpu()\n",
    "    img = img / 2 + 0.5   # unnormalize\n",
    "    with sns.axes_style(\"white\"):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img.permute((1, 2, 0)).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the \"transforms.Normalize\" the normalization of the data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# The output of torchvision datasets are PIL images in the range [0, 1]. \n",
    "# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # subtract 0.5 and divide by 0.5\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 64  # both for training and testing\n",
    "\n",
    "# Load datasets\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# Map from class index to class name.\n",
    "classes = {index: name for name, index in train_set.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Number of points: 50000\n",
      "Batch dimension (B x C x H x W): torch.Size([64, 3, 32, 32])\n",
      "Number of distinct labels: 10 (unique labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n",
      "\n",
      "Test data\n",
      "Number of points: 10000\n",
      "Batch dimension (B x C x H x W): torch.Size([64, 3, 32, 32])\n",
      "Number of distinct labels: 10 (unique labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\")\n",
    "print(\"Number of points:\", len(train_set))\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(train_set.targets))} (unique labels: {set(train_set.targets)})\")\n",
    "\n",
    "print(\"\\nTest data\")\n",
    "print(\"Number of points:\", len(test_set))\n",
    "x, y = next(iter(test_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(test_set.targets))} (unique labels: {set(test_set.targets)})\")\n",
    "\n",
    "n_classes = len(set(test_set.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try without shrinking the image, apply Kernels with the adecuate padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16, 16, 8, 8, 8, 4, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Image \n",
    "image_dim = 32\n",
    "chanels_in = 3\n",
    "\n",
    "# After applying 1st Kernel\n",
    "chanels_out1 = 32\n",
    "kernel1 = 9\n",
    "padd1 = 4\n",
    "image_dim1 = image_dim - kernel1 + 1 + 2*padd1\n",
    "\n",
    "# Max pool: 1st\n",
    "max_pool1 = 2\n",
    "image_dim1_p = image_dim1//max_pool1\n",
    "\n",
    "# After applying 2nd Kernel\n",
    "chanels_out2 = 64\n",
    "kernel2 = 7\n",
    "padd2 = 3\n",
    "image_dim2 = image_dim1_p - kernel2 + 1 + 2*padd2\n",
    "\n",
    "# Max pool: 2nd\n",
    "max_pool2 = 2\n",
    "image_dim2_p =  image_dim2//max_pool2\n",
    "\n",
    "# After applying 3th Kernel\n",
    "chanels_out3 = 128\n",
    "kernel3 = 5\n",
    "padd3 = 2\n",
    "image_dim3 = image_dim2_p - kernel3 + 1 + 2*padd3\n",
    "\n",
    "# kernel 4\n",
    "chanels_out4 = 256\n",
    "kernel4 = 3 \n",
    "padd4 = 1\n",
    "image_dim4 = image_dim3 - kernel4 + 1 + 2*padd4 \n",
    "\n",
    "# kernel 5\n",
    "chanels_out5 = 256\n",
    "kernel5 = 3 \n",
    "padd5 = 1\n",
    "image_dim5 = image_dim3 - kernel4 + 1 + 2*padd4 \n",
    "\n",
    "# Max pool: 3th\n",
    "max_pool3 = 2\n",
    "image_dim5_p = image_dim4//max_pool2\n",
    "\n",
    "# Calculate the number of neurons \n",
    "n_features = chanels_out4*(image_dim5_p)**2  # n_features it's the input in for the neral network\n",
    "hidden_units = [800, 400]\n",
    "final_neurons = 200\n",
    "\n",
    "image_dim1, image_dim1_p, image_dim2, image_dim2_p, image_dim3, image_dim4, image_dim5_p, n_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model2(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.2, inplace=False)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Flatten(start_dim=1, end_dim=-1)\n",
      "    (15): Linear(in_features=4096, out_features=800, bias=True)\n",
      "    (16): ReLU()\n",
      "    (17): Linear(in_features=800, out_features=400, bias=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.2, inplace=False)\n",
      "    (20): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Dropout(p=0.3, inplace=False)\n",
      "    (23): Linear(in_features=200, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PrintSize(nn.Module):\n",
    "    \"\"\"Utility module to print current shape of a Tensor in Sequential, only at the first pass.\"\"\"\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.first:\n",
    "            print(f\"Size: {x.size()}\")\n",
    "            self.first = False\n",
    "        return x\n",
    "\n",
    "class Model2(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = n_classes\n",
    "        activation_relu = nn.ReLU\n",
    "        activation_elu = nn.ELU\n",
    "        activation_sig = nn.Sigmoid\n",
    "        activation_sof = nn.Softmax\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            # CONVOLUTION PART\n",
    "            \n",
    "            # Conv 1st Kernel + ReLU \n",
    "            nn.Conv2d(chanels_in, chanels_out1, (kernel1, kernel1), padding=padd1),  # (num_chanels, out_chanels, kernel_size, stride, padding)\n",
    "            activation_relu(),\n",
    "            # Max Pooling + Conv 2nd Kernel + Relu\n",
    "            nn.MaxPool2d((max_pool1,max_pool1)),\n",
    "            nn.Conv2d(chanels_out1, chanels_out2, (kernel2, kernel2), padding=padd2),  # (num_chanels, out_chanels, kernel_size, stride, padding)\n",
    "            activation_relu(),\n",
    "            # Max Pooling + Conv 3th Kernel + Relu\n",
    "            nn.MaxPool2d((max_pool2, max_pool2)),\n",
    "            nn.Conv2d(chanels_out2, chanels_out3, (kernel3, kernel3), padding=padd3),  # (num_chanels, out_chanels, kernel_size, stride, padding)\n",
    "            activation_relu(),\n",
    "            # Conv 4th Kenrel + Relu + Dropout\n",
    "            nn.Conv2d(chanels_out3, chanels_out4, (kernel4, kernel4), padding=padd4),\n",
    "            activation_relu(),\n",
    "            nn.Dropout(p=0.2, inplace=False),\n",
    "            # Conv 5th Kenrel + Relu\n",
    "            nn.Conv2d(chanels_out4, chanels_out5, (kernel5,kernel5), padding=padd5),\n",
    "            activation_relu(),\n",
    "            # Max Pool and Flatten \n",
    "            nn.MaxPool2d((max_pool3, max_pool3)),  # Specify the size of the Kernel of the max pooling operation \n",
    "            nn.Flatten(),  # from (1, channels, height, width) to (1, channels * height * width)\n",
    "          \n",
    "            # DENSE NEURAL NETWORK\n",
    "\n",
    "            # Input layer  \n",
    "            nn.Linear(n_features, hidden_units[0]),  # (in_features, out_features)\n",
    "            activation_relu(),\n",
    "            # Hidden layer: 1\n",
    "            nn.Linear(hidden_units[0], hidden_units[1]),\n",
    "            activation_relu(),\n",
    "            nn.Dropout(p=0.2, inplace=False),\n",
    "            # # Hidden layer: 2\n",
    "            nn.Linear(hidden_units[1], final_neurons),\n",
    "            activation_relu(),\n",
    "            nn.Dropout(p=0.3, inplace=False),\n",
    "            # Output Layer\n",
    "            nn.Linear(final_neurons, self.num_classes),\n",
    "                # activation_sof(dim=1)   # Why we don't have to put the softmax? it's already defined ? \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model2 = Model2(n_classes)\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "model2.to(device)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10])\n",
      "Output logits:\n",
      "[[ 0.01902679 -0.03527096  0.02573777  0.05851943  0.06461272  0.04608821\n",
      "  -0.04030903  0.02836415  0.02209499 -0.05110846]\n",
      " [ 0.02148146 -0.05292613  0.03364832  0.05280799  0.06132663  0.04973081\n",
      "  -0.0392937   0.0365147   0.02143052 -0.04793116]]\n",
      "Output probabilities:\n",
      "[[0.1004485  0.0951398  0.10112488 0.10449485 0.10513351 0.10320389\n",
      "  0.09466168 0.10139082 0.10075717 0.09364489]\n",
      " [0.10069752 0.09347682 0.10193018 0.10390195 0.10479084 0.10358272\n",
      "  0.09475987 0.10222276 0.10069239 0.09394491]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model2(torch.randn(2, 3, 32, 32, device=device))\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n",
    "print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500     training accuracy: 0.2351875\n",
      "             test accuracy: 0.3344\n",
      "Step 1000    training accuracy: 0.4038130733944954\n",
      "             test accuracy: 0.3989\n",
      "Step 1500    training accuracy: 0.46309375\n",
      "             test accuracy: 0.496\n",
      "Step 2000    training accuracy: 0.5132955848623854\n",
      "             test accuracy: 0.5019\n",
      "Step 2500    training accuracy: 0.554890422077922\n",
      "             test accuracy: 0.5388\n",
      "Step 3000    training accuracy: 0.5671875\n",
      "             test accuracy: 0.573\n",
      "Step 3500    training accuracy: 0.599588373655914\n",
      "             test accuracy: 0.6004\n",
      "Step 4000    training accuracy: 0.6368055555555555\n",
      "             test accuracy: 0.6052\n",
      "Step 4500    training accuracy: 0.63075\n",
      "             test accuracy: 0.6013\n",
      "Step 5000    training accuracy: 0.6592938311688312\n",
      "             test accuracy: 0.619\n",
      "Step 5500    training accuracy: 0.6941105769230769\n",
      "             test accuracy: 0.6384\n",
      "Step 6000    training accuracy: 0.6811875\n",
      "             test accuracy: 0.6308\n",
      "Step 6500    training accuracy: 0.7009477459016393\n",
      "             test accuracy: 0.6423\n",
      "Step 7000    training accuracy: 0.69446875\n",
      "             test accuracy: 0.6591\n",
      "Step 7500    training accuracy: 0.7221320346320347\n",
      "             test accuracy: 0.6263\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "validation_every_steps = 500\n",
    "\n",
    "step = 0\n",
    "model2.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass, compute gradients, perform one training step.\n",
    "        \n",
    "        # Forward pass.\n",
    "        output = model2(inputs)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_fn(output, targets)\n",
    "        \n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Increment step counter\n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            valid_accuracies_batches = []\n",
    "            with torch.no_grad():\n",
    "                model2.eval()\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output = model2(inputs)\n",
    "                    loss = loss_fn(output, targets)\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "                model2.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e52dddd8ca2ba95afe67578a96296e9b17628fddb050e9ee950fdfaca96878c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
